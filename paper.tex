%%
%% This is file `sample-acmsmall-conf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `acmsmall-conf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-acmsmall-conf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[acmsmall,nonacm]{acmart}

\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{subfig}
\usepackage{glossaries}
%\usepackage{hyperref}

%% PlotNeuralNetwork stuff
%\usepackage{tikz}
%\usepackage{import}
%\subimport{figures/architectures/layers/}{init}
%\usetikzlibrary{positioning}
%\usetikzlibrary{3d} %for including external image 
%
%\def\ConvColor{rgb:yellow,5;red,2.5;white,5}
%\def\ConvReluColor{rgb:yellow,5;red,5;white,5}
%\def\PoolColor{rgb:red,1;black,0.3}
%\def\UnpoolColor{rgb:blue,2;green,1;black,0.3}
%\def\FcColor{rgb:blue,5;red,2.5;white,5}
%\def\FcReluColor{rgb:blue,5;red,5;white,4}
%\def\SoftmaxColor{rgb:magenta,5;black,7}   
%
%\newcommand{\copymidarrow}{\tikz \draw[-Stealth,line width=0.8mm,draw={rgb:blue,4;red,1;green,1;black,3}] (-0.3,0) -- ++(0.3,0);}
%

\newcommand\note[2]{{\color{#1}#2}}
\newcommand\todo[1]{{\note{red}{TODO: #1}}}
\usepackage[normalem]{ulem}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
%\AtBeginDocument{%
%  \providecommand\BibTeX{{%
%    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
%\setcopyright{acmcopyright}
%\copyrightyear{2018}
%\acmYear{2018}
%\acmDOI{10.1145/1122445.1122456}

%% These commands are for a PROCEEDINGS abstract or paper.
%\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
%  Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY}
%\acmPrice{15.00}
%\acmISBN{978-1-4503-9999-9/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{City-GAN: Learning architectural styles using a custom Conditional GAN architecture}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
%\author{Ben Trovato}
%\authornote{Both authors contributed equally to this research.}
%\email{trovato@corporation.com}
%\orcid{1234-5678-9012}
%\author{G.K.M. Tobin}
%\authornotemark[1]
%\email{webmaster@marysville-ohio.com}
%\affiliation{%
%  \institution{Institute for Clarity in Documentation}
%  \streetaddress{P.O. Box 1212}
%  \city{Dublin}
%  \state{Ohio}
%  \postcode{43017-6221}
%}
%
%\author{Lars Th{\o}rv{\"a}ld}
%\affiliation{%
%  \institution{The Th{\o}rv{\"a}ld Group}
%  \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%  \city{Hekla}
%  \country{Iceland}}
%\email{larst@affiliation.org}
%
%\author{Valerie B\'eranger}
%\affiliation{%
%  \institution{Inria Paris-Rocquencourt}
%  \city{Rocquencourt}
%  \country{France}
%}
%
%\author{Aparna Patel}
%\affiliation{%
% \institution{Rajiv Gandhi University}
% \streetaddress{Rono-Hills}
% \city{Doimukh}
% \state{Arunachal Pradesh}
% \country{India}}
%
%\author{Huifen Chan}
%\affiliation{%
%  \institution{Tsinghua University}
%  \streetaddress{30 Shuangqing Rd}
%  \city{Haidian Qu}
%  \state{Beijing Shi}
%  \country{China}}
%
%\author{Charles Palmer}
%\affiliation{%
%  \institution{Palmer Research Laboratories}
%  \streetaddress{8600 Datapoint Drive}
%  \city{San Antonio}
%  \state{Texas}
%  \postcode{78229}}
%\email{cpalmer@prl.com}

\author{Maximilian Bachl}
%\affiliation{\institution{The Th{\o}rv{\"a}ld Group}}
\email{maximilian.bachl@gmail.com}

\author{Daniel C. Ferreira}
%\affiliation{\institution{The Kumquat Consortium}}
\email{daniel.ferreira.1@gmail.com}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Bachl and Ferreira}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Generative Adversarial Networks (GANs) are a well-known technique that is trained on samples (e.g.~pictures of fruits) and which after training is able to generate realistic new samples. Conditional GANs (CGANs) additionally provide label information for subclasses (e.g.~apple, orange, pear) which enables the GAN to learn more easily and increase the quality of its output samples. We use GANs to learn architectural features of major cities and to generate images of buildings which do not exist. We show that currently available GAN and CGAN architectures are unsuited for this task and propose a custom architecture and demonstrate that our architecture has superior performance for this task and verify its capabilities with extensive experiments. 
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
%\begin{CCSXML}
%<ccs2012>
% <concept>
%  <concept_id>10010520.10010553.10010562</concept_id>
%  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%  <concept_significance>500</concept_significance>
% </concept>
% <concept>
%  <concept_id>10010520.10010575.10010755</concept_id>
%  <concept_desc>Computer systems organization~Redundancy</concept_desc>
%  <concept_significance>300</concept_significance>
% </concept>
% <concept>
%  <concept_id>10010520.10010553.10010554</concept_id>
%  <concept_desc>Computer systems organization~Robotics</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
% <concept>
%  <concept_id>10003033.10003083.10003095</concept_id>
%  <concept_desc>Networks~Network reliability</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
%</ccs2012>
%\end{CCSXML}

%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}
%\ccsdesc{Computer systems organization~Robotics}
%\ccsdesc[100]{Networks~Network reliability}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
%\keywords{datasets, neural networks, gaze detection, text tagging}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

%\begin{teaserfigure}
%  \includegraphics[width=\textwidth]{sampleteaser}
%  \caption{Seattle Mariners at Spring Training, 2010.}
%  \Description{Enjoying the baseball game from the third-base
%  seats. Ichiro Suzuki preparing to bat.}
%  \label{fig:teaser}
%\end{teaserfigure}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

%\begin{document}

%\maketitle

%\todo{\begin{enumerate}
%\item Create visualization of the neural network. I mean just a schematic overview.
%\item \sout{Show the actual filters of the CNN. I think there should be plenty of tools for that available. This should highlight or research contribution more I guess.} Doesn't work.
%\item \sout{Related work bla bla.}
%\item \sout{Some nice format. Maybe ACM conference standard template?}
%\item \sout{Show how difficult different cities are to learn.}
%\end{enumerate}}


\newacronym{gan}{GAN}{Generative Adversarial Network}
\newacronym{cgan}{CGAN}{Conditional Generative Adversarial Network}
\newacronym{cnn}{CNN}{Convolutional Neural Network}

\section{Introduction}

\glspl{gan} are a neural network architecture whose main purpose is generating previously unseen instances of data following a previously learned distribution. For example, they can be used to generate new handwritten digits or faces of people who do not exist \cite{goodfellow_generative_2014}. 

This works by having two separate neural networks, the generator and the discriminator. The generator's task is to create realistic samples and the discriminator's task is to determine whether a given sample is real or a fake from the generator. The input to the generator is a vector of noise, and its output a data sample. The discriminator's input is a data sample and its output is a decision whether the sample if real or fake. Over the course of training a \gls{gan}, the generator learns to create more realistic samples while the discriminator gets better at distinguishing real and fake instances. 

\glspl{cgan} \cite{mirza_conditional_2014} were introduced recently after \glspl{gan}. Their modification is an additional label input for both the generator and the discriminator. For example, for learning to generate digits, the labels would be digits from 0 to 9. For example, if one gave the label ``2'' to the generator it would try to generate a digit that resembles ``2''. The discriminator then would also get the label ``2'' so that it knows that the sample it is looking at should be ``2'' no matter if it is real or fake. 

\glspl{cgan} can be used for tagging images by utilizing the discriminator \cite{mirza_conditional_2014}. It is also possible to use the labels in the generator to transition between different cases \cite{gauthier_conditional_2015}, like for example transitioning from ``1'' to ``7''. 

Commonly, \glspl{cgan} are combined with \glspl{cnn} to be able to process image data \citep{radford_unsupervised_2015}. For the generator this is done by concatenating the label with the noise vector that is fed into the generator. For the discriminator the image is processed by a regular \gls{cnn}. After the \gls{cnn} there is one (or more than one) fully connected layer, which takes as its input the output of the \gls{cnn} as well as the label. Finally the discriminator outputs its decision regarding the realness of the sample. 

We present one new dataset for \glspl{gan}: The dataset consists of thousands of images of facades of buildings in various cities. Each image also has a label, which indicates the city of the building depicted. 

We show that both regular Convolutional \glspl{gan} as well as Convolutional \glspl{cgan} have significant shortcomings for our use case. We then propose a new custom architecture, which delivers higher-quality results as well as more stable convergence for our datasets. 

In summary, we make the following contributions:
\begin{itemize}
\item a new neural network architecture for Convolutional \glspl{cgan}
\item two datasets of facades
\item insight about the architectural features of the cities under study
\item program code that allows practitioners to reproduce our experiments and generate similar datasets
\end{itemize}

All our code and the dataset we used are available at \url{https://github.com/muxamilian/city-gan}.

\section{Overview}

We implemented \textit{City-GAN}, a \textit{Generative Adversarial Network} \citep{goodfellow_generative_2014} that can generate facades of buildings in cities it was trained on. 

To achieve this, we followed these steps: 
\begin{enumerate}
\item We chose four cities for our analysis: Vienna, Paris, Amsterdam and Manhattan.
\item We created a list of valid addresses in these cities.
\item We downloaded images from Google Street View from a randomly sampled subset of these addresses in each city. We chose the altitude angle of the camera to be 20 degrees. 
\item We filtered images which fulfilled the following criteria:
\begin{itemize}
\item It is possible to find a way from the left to the right edge of the image that only goes over facades and has no gaps in between.
\item The building does not show \textit{anomalous structures}. For example, we consider construction sites as anomalous because they do not represent the typical cityscape. 

\end{itemize}
\begin{figure}[h]
\includegraphics[width=0.49\columnwidth{}]{"figures/examples/1 Place de Mexico, 75016".jpg}%
\hspace{0.015\columnwidth}%
\includegraphics[width=0.49\columnwidth{}]{"figures/examples/1 Port de la Bourdonnais, 75007".jpg}
\caption{Two examples of pictures that we did not include in our dataset: on the left image there are gaps in the facades, the right one is anomalous (not a facade).}
\label{fig:example_invalid}
\end{figure}

\item After filtering we had 1000 images per city (4000 in total). 
\item We experimented with different types of GANs and different architectures.
\item We used another, larger dataset \citep{zamir_repo_2019} and checked whether a significantly higher number of samples improves the learning performance. This dataset consists of 500,000 facades each for Amsterdam, Washington DC, Florence, Las Vegas and Manhattan. These images are, however, not curated. The only restriction we made is that the altitude angle must be between 0 and 40 degrees. Otherwise, these images are randomly sampled from the cities with no further restrictions. 
\end{enumerate}

We tried different GAN architectures during the course of our experimentation. For all our experiments we used the following data augmentation methods: horizontal flipping with probability 0.5 as well as random cropping of approximately 85\% of the original width and height of the images. We experimented with different cropped image sizes: 64, 128 and 256 pixels in height and width. For a final image of 256 pixels, we would input an image of 300 pixels and randomly crop it to 256.

\section{Regular GAN}

First, we used a regular deep convolutional GAN \citep{radford_unsupervised_2015} and didn't distinguish between the different cities. The code we used we got from the official PyTorch examples \citep{pytorch_deep_2019} which implements the procedure outlined by \citet{radford_unsupervised_2015}. However, from visually inspecting the results, we concluded that the GAN has a hard time learning the data distribution. 

The generator starts with a vector of multivariate gaussian noise (length 100) and continuously deconvolutes it (kernel size 4, stride 2) until it outputs the image of the final size. 

The discriminator gets the image and continuously convolutes it (kernel size 4, stride 2) until it is of dimension $1\times 1 \times n_\text{channels}$. Then there is one linear layer followed by a sigmoid function that outputs whether the sample is real or fake. 

\begin{figure}[h]
\includegraphics[width=\columnwidth]{figures/gan.jpg}
\caption{Eight random samples from the generator of the regular GAN.}
\label{fig:gan}
\end{figure}

\autoref{fig:gan} shows the results of our generator after sufficient training iterations. We see that while its output looks like facades, the output is clearly faulty and can be easily characterized by a human as being fake. 

\section{Conditional GAN 1}

After realizing that a regular GAN has problems learning the data distribution we used a conditional GAN (CGAN) \citep{mirza_conditional_2014}. The rationale behind this was that if we tell the GAN which city a picture is from, it can focus on learning facades and doesn't get confused by different cities' styles. 
The difference between a GAN and a CGAN is that the latter receives label information both in the generator and discriminator.
As such, the discriminator has more information to distinguish between real and fake pictures, and the generator has an incentive to develop features specifically for each label.

We implemented it ourselves based on the code of the regular GAN. The generator of the CGAN gets a one-hot-encoded vector of the cities alongside the multivariate Gaussian distribution and can thus consider the additional information about the city that the discriminator expects in the image generation process. For instance, when the generator is supposed to output an image of Amsterdam, it gets the vector $\begin{pmatrix}1, 0, 0, 0 \end{pmatrix}^\top$ attached to its input. Thus the input to the generator network is a vector of length 104 with our four cities (100+4). 

The discriminator of the GAN proceeds as in the regular GAN and uses a convolutional neural network to process the input image. After the convolutional layers we concatenate their output with the one-hot-encoded vector of the city and pass it through a dense layer followed by a ReLU function. After that we have another dense layer followed by a sigmoid function that outputs whether the sample is real or fake.  

\begin{figure}[h]
\includegraphics[width=\columnwidth]{figures/daniel_net.jpg}
\caption{Our first attempt at a CGAN. Each column represents one random input to the generator. The rows represent: the average of all cities, Amsterdam, Manhattan, Paris and Vienna. }
\label{fig:daniel_net}
\end{figure}

\autoref{fig:daniel_net} shows that the results are even worse than for the regular GAN.

\section{Conditional GAN with custom architecture} 

After achieving suboptimal results with this CGAN architecture we tried a different approach and modified the discriminator: instead of adding the one-hot-encoded vector with the city information after the convolution, we decided to add it as an input to each pixel in the beginning of the convolution. Thus, additional to the RGB values, each pixel also has the one-hot-encoded information of the city. For example, with four cities, each pixel has 7 color channels: 3 for the colors and 4 for the one-hot-encoded cities (\autoref{fig:maxnet}). 

\begin{figure}
\includegraphics[width=\columnwidth]{figures/architectures/maxnet.pdf}
\caption{The architecture of our neural network is inspired by \cite{pytorch_deep_2019}. The difference in our custom architecture is that labels are fed into the generator (top) as well as into the discriminator (bottom) alongside the data.}  % TODO
\label{fig:maxnet}
\end{figure}

While this approach sounds wasteful because of the duplicate information that is fed into the convolutional layer, it achieved significantly better results. One possible explanation for this improvement is that by adding the city information to the input picture, we help the kernels of the convolution to work differently for each city. 

\begin{figure}[h]
\includegraphics[width=\columnwidth]{figures/good_hack_final.jpg}
\caption{Our custom CGAN architecture. Each column represents one random input to the generator. The rows represent: the average of all cities, Amsterdam, Manhattan, Paris and Vienna.}
\label{fig:hack}
\end{figure}

\autoref{fig:hack} shows that the results of our custom CGAN are significantly better than with our previous attempt and are also better than the normal GAN. 

\section{Influence of image size on the quality of generated images}

We also tested the effect of image size on the quality of the generated images. For this, we used images with a length and width of 64, 128 and 256 pixels. Our general conclusion is that smaller image sizes improve quality: with 64 pixels \autoref{fig:smaller}, the generated facades looked good and had no or only very minor flaws. With 128 we got a larger number of artifacts and faulty images but we deemed quality to be still acceptable. With 256, we couldn't get satisfying results any longer: the generator never converged and changed its output over time when fed the same random input. Furthermore, the generator did not generalize over different cities and, when inputting an average of all cities using the input vector $\begin{pmatrix}\frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4}\end{pmatrix}^\top$---which gives equal importance to all cities---the obtained image was only noise. Thus, the generator only learned to output facades for each city but did not learn which features all cities have in common. During the course of our further experiments we only used images of resolution $128\times 128$.

\begin{figure}[h]
\centering
\includegraphics[width=0.75\linewidth]{figures/good_hack_64.jpg}
\caption{Our custom CGAN trained on images of size $64\times 64$. Each column represents one random input to the generator. The rows represent: the average of all cities, Amsterdam, Manhattan, Paris and Vienna.}
\label{fig:smaller}
\end{figure}

\section{Using a larger, uncurated dataset}

Finally, we wanted to investigate whether a larger dataset could significantly improve the quality of generated images. For this we used a dataset of Stanford University \citep{pytorch_deep_2019} and extracted 500,000 images for the cities of Amsterdam, Washington DC, Florence, Las Vegas and Manhattan. These images are not curated, thus it is not guaranteed that they actually show facades. 

%\begin{figure}[h]
%\centering
%\subfloat[Uncurated dataset]{\includegraphics[width = 0.49\linewidth]{figures/probability_all.pdf}} 
%\subfloat[Curated dataset]{\includegraphics[width = 0.49\linewidth]{figures/probability_europe.pdf}} \caption{The probability for real images being real (green) and fake images being real (red) of the discriminator after completing training.}
%\label{fig:probabilities}
%\end{figure}

We noticed that Washington DC and Las Vegas are especially problematic for our neural network. 
In Washington DC, the majority of images depict trees, and in Las Vegas there are few multi-story buildings. 
%\autoref{fig:probabilities} shows the probabilities for real images being deemed real and fake images being deemed real by the discriminator after finishing training. In the ideal case, the probability of both would be 0.5 meaning that the discriminator cannot distinguish between real and fake images anymore because the generator is performing so well. It shows that for the curated dataset, the gap between real and fake image probabilities as about the same, while for the uncurated dataset some cities like Florence and Las Vegas seem to be especially hard for the generator. For Washington D.C. the gap is surprisingly small. We attribute this to the fact that on the images of Washington D.C. one can mostly see 

Thus we also tried training the CGAN with only these three cities, hoping this would increase the quality of learned images.

\begin{figure}[h]
\includegraphics[width=\columnwidth]{figures/good_hack_stanford_dataset.jpg}
\caption{Our custom CGAN trained on the Stanford dataset. Each column represents one random input to the generator. The rows represent: the average of all cities, Amsterdam, Manhattan, Paris and Vienna.}
\label{fig:stanford_net}
\end{figure}

Generally speaking we think that the quality of the generated images is the same as the one of our own dataset \autoref{fig:stanford_net}. However, while with our dataset we only learn facades, with the Stanford dataset we also learn entire streets with building on the left and right side of the street. 

\section{Traveling Between Cities}

To understand if our network learns characteristics of each city, we can compare the output of the generator for the same vector with different labels.
We show a 5-step interpolation for 3 distinct random vectors in Figure~\ref{fig:transfer}.
We show results interpolating from Amsterdam to each one of Florence, Washington DC and Manhattan.
Additionally, we show also results of interpolating from ``Europe'' (half Amsterdam/half Florence) to the ``US'' (a third of each American city).
These results are from the CGAN with the uncurated dataset described in the last section.

\begin{figure}[h]
	\centering
	\subfloat[Amsterdam$\to$Florence]{\includegraphics[width = 0.49\linewidth]{figures/amsterdam_florence/merged}\label{fig:ta}}
	\hspace{0.01\linewidth}
	\subfloat[Amsterdam$\to$Washington DC]{\includegraphics[width = 0.49\linewidth]{figures/amsterdam_dc/merged}\label{fig:tb}}\\
	\subfloat[Amsterdam$\to$Manhattan]{\includegraphics[width = 0.49\linewidth]{figures/amsterdam_manhattan/merged}\label{fig:tc}}
	\hspace{0.01\linewidth}
	\subfloat[Europe$\to$US]{\includegraphics[width = 0.49\linewidth]{figures/europe_us/merged}\label{fig:td}} 
	\caption{Outputs of our CGAN when interpolating between locations.
	Each row contains one cityscape, interpolated across the locations specified in the subtitle.
}
	\label{fig:transfer}
\end{figure}

The first thing we notice is that the color palette of the cities is quite different.
For example, Amsterdam is quite redish, while Florence has tones closer to beige (\autoref{fig:ta}).
Another obvious characteristic is that Washington DC has a lot of trees (\autoref{fig:tb}).
In all the examples with it, the amount of trees greatly increases when compared to Amsterdam.

There are also some more nuanced changes that characterize the stylistic architectural differences between cities.
A good example of this is show between Amsterdam and Florence: distinct roofs, with breaks between them can be seen.
This is most noticeable in the second row of \autoref{fig:ta}.
Between Amsterdam and Manhattan there are also two strong distinctions: Manhattan has taller buildings, with more smaller windows.
In the second row of Figure~\ref{fig:tc}, the buildings grow as the label approaches Manhattan, and in the third row the windows split into smaller windows.

We also show the transition between Europe and the US.
The US images look quite unrealistic, as they are an average between 3 cities with quite different styles.
However, one can identify an increase of trees (Washington DC has really a lot of trees) and different angles of photography (buildings are farther from the camera in Washington DC and Las Vegas).
Both of these differences are evident in the second row of \autoref{fig:td}.
Another good example of the Europe/US relation is in \autoref{fig:skyscraperchurch}, which shows an European style church being transformed into an American style skyscraper.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.49\linewidth]{figures/europe_us/44}
	\caption{Transition from a tall European church to an American skyscraper.}
	\label{fig:skyscraperchurch}
\end{figure}

Another thing we can try is to experiment with subtracting stylistic features.
We can for example subtract Manhattan's label from Amsterdam's, and intuitively that should remove typical Manhattan features from the Amsterdam image.
Indeed, we do just that in \autoref{fig:minusmanhattan}, and observe that the modern looking building in the background in Amsterdam completely disappears, after subtracting Manhattan. Also, if you subtract Manhattan, more trees appear. Apparently, in Manhattan suppresses the output of trees because there are not many there. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.49\linewidth]{figures/amsterdam_minus_manhattan}
	\caption{Transition from Amsterdam to Amsterdam-Manhattan (This means that we set the one-hot vector with 1 for Amsterdam and -1 for Manhattan).}
	\label{fig:minusmanhattan}
\end{figure}

Not all images obtained are realistic, and some are even quite bad.
In Figure~\ref{fig:bad}, we show some of these bad examples.
The transitions are in the same order as in \autoref{fig:transfer}, but with only one fixed random vector instead of 3.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/bad/merged_names}
	\caption{Transition from Amsterdam to each of Florence, Washington DC, and Manhattan (first 3 rows), and from Europe to the US (last row).}
	\label{fig:bad}
\end{figure}

\section{Conclusion}

In this paper we sought to generate images of facades containing the stylistic features of specific cities.
We experimented with multiple GAN architectures, and propose a custom architecture.
Our custom architecture produced more realistic images than the alternative architectures, and was capable of generating architectural features specific to each city.
By using a continuous label space, we were also able to transition one facade between cities, or do averages over them.

The results suggest that our architecture is also better suited for other tasks than the existing architectures.
Investigating its effects on other tasks is a feasible future research path.

We have made all our code and used data available, so that the community can both reproduce and improve on our results.

\section*{Acknowledgement}
% We thank NVIDIA Corporation for donating a Titan Xp.
% The Titan Xp we used was donated by NVIDIA.
The Titan Xp used for this research was donated by the NVIDIA Corporation.

%\bibliographystyle{plainnat}
\bibliographystyle{ACM-Reference-Format}
\bibliography{biblio}

\end{document}